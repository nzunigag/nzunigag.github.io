---
layout: post
title: "Lasso Regression with R"
featured-img: lasso
categories: ['#StatsIsFun']
mathjax: true 
---

[This post is still under development :/]

This is the first post I prepare for a Data Science Topic. I want to take this opportunity to review key concepts in Statistics by writing examples that can be usefull for others. Feel free to [contact](https://nzunigag.github.io/contact/) me with questions or suggestions.

# Lasso Regression
Consider the standard linear regression model

$$
y = X \beta + e \, ,
$$

where $y$ is an $n$-vector of responses, $X$ is an $n \times p$ features matrix whose $i$th row $x_i$ is the vector of features for observation $i$, and $e$ is a vector of errors/residuals.

The lasso involves estimating $\beta$ as the solution to the penalized least-squares problem

$$
\hat{\beta} = \arg \min_{\beta} \; \frac{1}{2} \Vert y - X \beta \Vert_2^2 + \lambda \Vert \beta \Vert_1 \, ,
$$

where $\Vert \beta \Vert_1$ is the $\ell_1$ (pronounced ``ell one'') norm of the coefficient vector:

$$
\Vert \beta \Vert_1 = \sum_{j=1}^p |\beta_j| \, .
$$

The penalty function is just like the absolute-value penalty you used on the normal-means problem, generalized to the vector case.  And just as in in the normal means problem, this penalty function will have the effect of both selecting a set of nonzero $\beta_i$'s (i.e.~sparsifying the estimate) as well as shrinking the nonzero $\beta_i$'s toward 0.  The bigger $\lambda$, the more aggressive the shrinkage effect.

Note: we typically leave the intercept in a lasso fit unpenalized.  We can accomplish this by explicitly introducing an intercept, e.g. writing the objective as

$$
\frac{1}{2n} \Vert y - (\alpha \mathbf{1} + X \beta) \Vert_2^2 + \lambda \Vert \beta \Vert_1 \, ,
$$

where $\alpha$ is a scalar intercept and $\mathbf{1}$ is a vector of all 1's.  Or we can leave the problem in its original form above, and assume that both the response variable $y$ and all columns of the predictor matrix have been standardized have a mean of 0 (in which case there is no need for an explicit intercept).  For the rest of these exercises, we'll assume that the variables have been standardized in this way.

To read more about lasso regression, consult Chapter 3.4.2 of The Elements of Statistical Learning, or the [original paper](http://statweb.stanford.edu/~tibs/lasso/lasso.pdf) by Robert Tibshirani.


### Acknowledgment 

This post is made using material from the Big Data course I took during my master's in Statistics at UT Austin. Feel free to check the content of the course in this [GitHub page](https://github.com/jgscott/SDS385) or visit my [personal GitHub](https://github.com/nzunigag/SDS-385-Statistical-Models-for-Big-Data) that includes my solutions to the course exercises.

